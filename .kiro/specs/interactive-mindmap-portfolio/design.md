# Design Document

## Overview

ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒã‚¤ãƒ³ãƒ‰ãƒãƒƒãƒ—å½¢å¼ã®ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚µã‚¤ãƒˆã¯ã€Yewï¼ˆRust WebAssemblyï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨Rapier2Dç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ã¦æ§‹ç¯‰ã•ã‚Œã¾ã™ã€‚ç¾åœ¨ã®å®Ÿè£…ã‚’æ‹¡å¼µã—ã€Markdownãƒ™ãƒ¼ã‚¹ã®è¨˜äº‹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã¨ã€å‹•çš„ãªãƒãƒ¼ãƒ‰è¡¨ç¤ºã‚·ã‚¹ãƒ†ãƒ ã‚’çµ±åˆã—ã¾ã™ã€‚

### æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
- **ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰**: Yew (Rust WebAssembly)
- **ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³**: Rapier2D
- **ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**: yew-router
- **Markdownå‡¦ç†**: pulldown-cmark
- **ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°**: ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³CSSï¼ˆå°†æ¥çš„ã«CSS-in-Rustãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ¤œè¨ï¼‰
- **CI/CD**: GitHub Actions
- **è¨˜äº‹ç®¡ç†**: Rust CLIãƒ„ãƒ¼ãƒ«ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã€ãƒªãƒ³ã‚¯è§£æã€é–¢é€£æ€§è¨ˆç®—ï¼‰
- **ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ**: é™çš„JSONç”Ÿæˆï¼ˆãƒ“ãƒ«ãƒ‰æ™‚ãƒ»ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ï¼‰
- **é–‹ç™ºç’°å¢ƒ**: Nixï¼ˆä¾å­˜é–¢ä¿‚ç®¡ç†ã€å†ç¾å¯èƒ½ãªãƒ“ãƒ«ãƒ‰ç’°å¢ƒï¼‰
- **ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º**: Rust CLIãƒ„ãƒ¼ãƒ« + Nix shellï¼ˆçµ±ä¸€çš„ãªé–‹ç™ºä½“é¨“ï¼‰

### é‡è¦ãªé–‹ç™ºç’°å¢ƒã®æ³¨æ„äº‹é …

**Nixç’°å¢ƒã§ã®ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ**
- ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯Nix flakeã‚’ä½¿ç”¨ã—ã¦é–‹ç™ºç’°å¢ƒã‚’ç®¡ç†ã—ã¦ã„ã¾ã™
- å…¨ã¦ã®Rustã‚³ãƒãƒ³ãƒ‰ï¼ˆcargo buildã€cargo testã€cargo runãªã©ï¼‰ã¯Nixç’°å¢ƒå†…ã§å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
- **é‡è¦**: `nix develop`ã‚’å˜ç‹¬ã§å®Ÿè¡Œã™ã‚‹ã¨ã€Kiro IDEã§ã¯çµ‚äº†åˆ¤å®šãŒæ­£ã—ãè¡Œã‚ã‚Œã¾ã›ã‚“
- **æ¨å¥¨æ–¹æ³•**: `nix develop --command [å®Ÿè¡Œã—ãŸã„ã‚³ãƒãƒ³ãƒ‰]`ã®å½¢å¼ã§ãƒ¯ãƒ³ãƒ©ã‚¤ãƒŠãƒ¼ã¨ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„

**ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œä¾‹**:
```bash
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
nix develop --command cargo test

# ãƒ“ãƒ«ãƒ‰å®Ÿè¡Œ
nix develop --command cargo build

# è¨˜äº‹å‡¦ç†ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
nix develop --command cargo run --bin process-articles

# é–‹ç™ºã‚µãƒ¼ãƒãƒ¼èµ·å‹•
nix develop --command trunk serve

# justã‚³ãƒãƒ³ãƒ‰ä½¿ç”¨
nix develop --command just dev
```

ã“ã®æ–¹å¼ã«ã‚ˆã‚Šã€Nixç’°å¢ƒã®ä¾å­˜é–¢ä¿‚ã‚’æ­£ã—ãåˆ©ç”¨ã—ãªãŒã‚‰ã€Kiro IDEã§ã®é–‹ç™ºã‚’å††æ»‘ã«é€²ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

## Architecture

### å…¨ä½“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GitHub Repository                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GitHub Actions Workflows                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Article         â”‚  â”‚ Link Graph      â”‚  â”‚ Metadata     â”‚ â”‚
â”‚  â”‚ Processing      â”‚  â”‚ Generator       â”‚  â”‚ Validator    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                     â”‚                   â”‚       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Markdown Files  â”‚  â”‚ Generated JSON  â”‚  â”‚ Build        â”‚ â”‚
â”‚  â”‚ (articles/*.md) â”‚  â”‚ (data/*.json)   â”‚  â”‚ Artifacts    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Browser (WebAssembly)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Yew Application                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Router        â”‚  â”‚  Home Component â”‚  â”‚ Article View â”‚ â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                     â”‚                   â”‚       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Data Loader     â”‚  â”‚ Node Graph      â”‚  â”‚ Enhanced     â”‚ â”‚
â”‚  â”‚ (JSON Consumer) â”‚  â”‚ Container       â”‚  â”‚ Markdown     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ Renderer     â”‚ â”‚
â”‚           â”‚                     â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Static Data     â”‚  â”‚ Physics World   â”‚  â”‚ Interactive  â”‚ â”‚
â”‚  â”‚ (Pre-processed) â”‚  â”‚                 â”‚  â”‚ Navigation   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Static Assets                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Article Data    â”‚  â”‚ Link Graph      â”‚  â”‚ Images       â”‚ â”‚
â”‚  â”‚ (articles.json) â”‚  â”‚ (links.json)    â”‚  â”‚ (assets/*)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

#### ãƒ“ãƒ«ãƒ‰æ™‚ï¼ˆGitHub Actions / ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºï¼‰
1. **è¨˜äº‹æ¤œå‡º**: Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ã‚’æ¤œå‡ºï¼ˆpush/PRæ™‚ or ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ï¼‰
2. **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º**: Front matterã¨Markdownå†…å®¹ã‚’è§£æ
3. **ãƒªãƒ³ã‚¯è§£æ**: è¨˜äº‹é–“ã®ãƒªãƒ³ã‚¯é–¢ä¿‚ã‚’æŠ½å‡ºãƒ»æ¤œè¨¼
4. **JSONç”Ÿæˆ**: å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’é™çš„JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›
5. **æ¤œè¨¼**: ãƒªãƒ³ã‚¯åˆ‡ã‚Œã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¸æ•´åˆã‚’ãƒã‚§ãƒƒã‚¯ãƒ»å ±å‘Š

#### ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ï¼‰
1. **ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿**: äº‹å‰ç”Ÿæˆã•ã‚ŒãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
2. **ãƒãƒ¼ãƒ‰ç”Ÿæˆ**: home_display=trueã®è¨˜äº‹ã‹ã‚‰ãƒãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
3. **ç‰©ç†æ¼”ç®—**: Rapier2Dã‚¨ãƒ³ã‚¸ãƒ³ãŒãƒãƒ¼ãƒ‰ã®ä½ç½®ã‚’è¨ˆç®—ã€ãƒªãƒ³ã‚¯é–¢ä¿‚ã«åŸºã¥ãæ¥ç¶šåŠ›ã‚’é©ç”¨
4. **ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°**: Yewã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒç‰©ç†æ¼”ç®—çµæœã‚’åŸºã«UIã‚’æç”»ã€æ¥ç¶šç·šã‚’ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º
5. **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³**: ãƒãƒ¼ãƒ‰ã‚¯ãƒªãƒƒã‚¯æ™‚ã«é–¢é€£è¨˜äº‹ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆã€è¨˜äº‹å†…ãƒªãƒ³ã‚¯ã§å‹•çš„é·ç§»
6. **ãƒ‡ãƒãƒƒã‚°UI**: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ã«ãƒãƒ¼ãƒ‰é–“ã®çµåˆåŠ›èª¿æ•´UIã‚’è¡¨ç¤º

#### ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
1. **è¨˜äº‹ä½œæˆãƒ»ç·¨é›†**: `articles/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†
2. **ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†å®Ÿè¡Œ**: `npm run process-articles`ã¾ãŸã¯`cargo run --bin process-articles`
3. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°**: ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã«ã‚ˆã‚‹è‡ªå‹•å†å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
4. **é–‹ç™ºã‚µãƒ¼ãƒãƒ¼èµ·å‹•**: ç”Ÿæˆã•ã‚ŒãŸJSONã‚’ä½¿ç”¨ã—ã¦Yewã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ

## Nix Development Environment

### 1. Nix Flake Configuration

#### flake.nix
```nix
{
  description = "Interactive Mindmap Portfolio";

  inputs = {
    nixpkgs.url = "github:NixOS/nixpkgs/nixos-unstable";
    rust-overlay.url = "github:oxalica/rust-overlay";
    flake-utils.url = "github:numtide/flake-utils";
  };

  outputs = { self, nixpkgs, rust-overlay, flake-utils }:
    flake-utils.lib.eachDefaultSystem (system:
      let
        overlays = [ (import rust-overlay) ];
        pkgs = import nixpkgs {
          inherit system overlays;
        };
        
        rustToolchain = pkgs.rust-bin.stable.latest.default.override {
          extensions = [ "rust-src" "rust-analyzer" ];
          targets = [ "wasm32-unknown-unknown" ];
        };
      in
      {
        devShells.default = pkgs.mkShell {
          buildInputs = with pkgs; [
            # Rust toolchain
            rustToolchain
            
            # WebAssembly tools
            wasm-pack
            trunk
            
            # Development tools
            watchexec
            just
            
            # System dependencies
            pkg-config
            openssl
          ];
          
          shellHook = ''
            echo "ğŸ¦€ Rust WebAssembly development environment"
            echo "ğŸ“¦ Available commands:"
            echo "  just dev      - Start development server with file watching"
            echo "  just build    - Build for production"
            echo "  just process  - Process articles and generate data"
            echo "  just validate - Validate links and content"
            echo "  just clean    - Clean generated files"
          '';
        };
        
        packages.default = pkgs.rustPlatform.buildRustPackage {
          pname = "khimoo-portfolio";
          version = "0.1.0";
          src = ./.;
          cargoLock.lockFile = ./Cargo.lock;
          
          buildInputs = with pkgs; [ pkg-config openssl ];
          
          # WebAssembly build
          buildPhase = ''
            cargo build --release
            wasm-pack build --target web --out-dir pkg
          '';
        };
      });
}
```

### 2. Rust CLI Tools

#### Cargo.toml
```toml
[package]
name = "khimoo-portfolio"
version = "0.1.0"
edition = "2021"

# Main application
[[bin]]
name = "khimoo-portfolio"
path = "src/main.rs"

# Article processing tools
[[bin]]
name = "process-articles"
path = "src/bin/process_articles.rs"

[[bin]]
name = "validate-links"
path = "src/bin/validate_links.rs"

[[bin]]
name = "generate-link-graph"
path = "src/bin/generate_link_graph.rs"

[[bin]]
name = "dev-server"
path = "src/bin/dev_server.rs"

[dependencies]
# Web framework
yew = { version = "0.21", features = ["csr"] }
yew-router = "0.18"
web-sys = { version = "0.3", features = ["HtmlElement", "HtmlDivElement", "Element", "DomRect"] }
yew-hooks = "0.3"

# Physics
rapier2d = { version = "0.26", features = ["simd-stable"] }

# Markdown processing
pulldown-cmark = "0.10"

# CLI tools dependencies
clap = { version = "4.0", features = ["derive"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
serde_yaml = "0.9"
walkdir = "2.3"
regex = "1.7"
notify = "6.0"
tokio = { version = "1.0", features = ["full"] }
anyhow = "1.0"
chrono = { version = "0.4", features = ["serde"] }

[dev-dependencies]
wasm-bindgen-test = "0.3"
```

#### justfile (Task Runner)
```just
# Default recipe
default:
    @just --list

# Start development environment
dev:
    @echo "ğŸš€ Starting development environment..."
    @just process-articles
    @watchexec -w articles -e md -- just process-articles &
    @trunk serve

# Process all articles
process-articles:
    @echo "ğŸ“ Processing articles..."
    @cargo run --bin process-articles

# Validate links
validate-links:
    @echo "ğŸ”— Validating links..."
    @cargo run --bin validate-links

# Generate link graph
generate-link-graph:
    @echo "ğŸ•¸ï¸  Generating link graph..."
    @cargo run --bin generate-link-graph

# Build all data
build-data: process-articles validate-links generate-link-graph
    @echo "âœ… All data processed successfully"

# Build for production
build: build-data
    @echo "ğŸ—ï¸  Building for production..."
    @trunk build --release

# Clean generated files
clean:
    @echo "ğŸ§¹ Cleaning up..."
    @rm -rf dist data/*.json target pkg

# Run tests
test:
    @echo "ğŸ§ª Running tests..."
    @cargo test
    @wasm-pack test --headless --firefox

# Watch articles and rebuild
watch:
    @echo "ğŸ‘€ Watching articles for changes..."
    @watchexec -w articles -e md -- just build-data

# Development server with hot reload
serve: build-data
    @echo "ğŸŒ Starting development server..."
    @trunk serve --open
```

#### Article Processing CLI
```rust
// src/bin/process_articles.rs
use clap::Parser;
use anyhow::{Context, Result};
use std::path::PathBuf;
use serde::{Deserialize, Serialize};
use walkdir::WalkDir;
use regex::Regex;
use std::collections::HashMap;
use chrono::{DateTime, Utc};

#[derive(Parser)]
#[command(name = "process-articles")]
#[command(about = "Process articles and generate static data")]
struct Args {
    /// Articles directory path
    #[arg(short, long, default_value = "articles")]
    articles_dir: PathBuf,
    
    /// Output directory for generated data
    #[arg(short, long, default_value = "data")]
    output_dir: PathBuf,
    
    /// Enable verbose logging
    #[arg(short, long)]
    verbose: bool,
}

#[tokio::main]
async fn main() -> Result<()> {
    let args = Args::parse();
    
    let processor = ArticleProcessor::new(args.articles_dir, args.output_dir, args.verbose);
    processor.process_all_articles().await
}

pub struct ArticleProcessor {
    articles_dir: PathBuf,
    output_dir: PathBuf,
    verbose: bool,
}

impl ArticleProcessor {
    pub fn new(articles_dir: PathBuf, output_dir: PathBuf, verbose: bool) -> Self {
        Self { articles_dir, output_dir, verbose }
    }

    pub async fn process_all_articles(&self) -> Result<()> {
        if self.verbose {
            println!("ğŸ”„ Processing articles from {:?}", self.articles_dir);
        }
        
        // Create output directory
        std::fs::create_dir_all(&self.output_dir)
            .context("Failed to create output directory")?;
        
        // Load and parse all articles
        let articles = self.load_and_parse_articles()
            .context("Failed to load articles")?;
        
        if self.verbose {
            println!("ğŸ“š Found {} articles", articles.len());
        }
        
        // Build link graph
        let link_graph = self.build_link_graph(&articles)
            .context("Failed to build link graph")?;
        
        // Validate content
        let validation_report = self.validate_content(&articles, &link_graph)
            .context("Failed to validate content")?;
        
        // Write output files
        self.write_articles_data(&articles)
            .context("Failed to write articles data")?;
        self.write_link_graph_data(&link_graph)
            .context("Failed to write link graph data")?;
        self.write_validation_report(&validation_report)
            .context("Failed to write validation report")?;
        
        println!("âœ… Successfully processed {} articles", articles.len());
        
        if !validation_report.errors.is_empty() {
            println!("âš ï¸  Found {} validation errors", validation_report.errors.len());
            for error in &validation_report.errors {
                println!("   - {}: {} -> {}", error.error_type, error.source, error.target);
            }
        }
        
        Ok(())
    }

    fn load_and_parse_articles(&self) -> Result<Vec<ProcessedArticle>> {
        let mut articles = Vec::new();
        
        for entry in WalkDir::new(&self.articles_dir)
            .into_iter()
            .filter_map(|e| e.ok())
            .filter(|e| e.path().extension().map_or(false, |ext| ext == "md"))
        {
            let content = std::fs::read_to_string(entry.path())
                .with_context(|| format!("Failed to read file: {:?}", entry.path()))?;
            
            let article = self.parse_article(entry.path(), &content)
                .with_context(|| format!("Failed to parse article: {:?}", entry.path()))?;
            
            articles.push(article);
        }
        
        Ok(articles)
    }

    fn parse_article(&self, file_path: &std::path::Path, content: &str) -> Result<ProcessedArticle> {
        // Parse front matter
        let (metadata, markdown_content) = self.parse_front_matter(content)?;
        
        // Extract links
        let outbound_links = self.extract_links(&markdown_content)?;
        
        // Extract tags from content
        let extracted_tags = self.extract_tags(&markdown_content);
        
        // Generate slug from file path
        let slug = self.generate_slug(file_path);
        
        Ok(ProcessedArticle {
            slug,
            title: metadata.title.clone(),
            content: markdown_content,
            metadata,
            file_path: file_path.to_string_lossy().to_string(),
            outbound_links,
            inbound_count: 0, // Will be calculated later
            extracted_tags,
            processed_at: Utc::now().to_rfc3339(),
        })
    }

    fn parse_front_matter(&self, content: &str) -> Result<(ProcessedMetadata, String)> {
        if !content.starts_with("---\n") {
            return Ok((ProcessedMetadata::default(), content.to_string()));
        }
        
        let end_marker = content[4..].find("\n---\n")
            .ok_or_else(|| anyhow::anyhow!("Invalid front matter: missing end marker"))?;
        
        let yaml_content = &content[4..end_marker + 4];
        let markdown_content = &content[end_marker + 8..];
        
        let metadata: ProcessedMetadata = serde_yaml::from_str(yaml_content)
            .context("Failed to parse YAML front matter")?;
        
        Ok((metadata, markdown_content.to_string()))
    }

    fn extract_links(&self, content: &str) -> Result<Vec<ProcessedLink>> {
        let mut links = Vec::new();
        
        // Extract [[wiki-style]] links
        let wiki_regex = Regex::new(r"\[\[([^\]]+)\]\]")?;
        for cap in wiki_regex.captures_iter(content) {
            let target = cap.get(1).unwrap().as_str();
            let position = cap.get(0).unwrap().start();
            
            links.push(ProcessedLink {
                target_slug: self.generate_slug_from_title(target),
                link_type: LinkType::WikiLink,
                context: self.get_context(content, position, 50),
                position,
            });
        }
        
        // Extract [text](slug) links
        let markdown_regex = Regex::new(r"\[([^\]]+)\]\(([^)]+)\)")?;
        for cap in markdown_regex.captures_iter(content) {
            let target = cap.get(2).unwrap().as_str();
            let position = cap.get(0).unwrap().start();
            
            // Only process internal links (not starting with http)
            if !target.starts_with("http") {
                links.push(ProcessedLink {
                    target_slug: target.to_string(),
                    link_type: LinkType::MarkdownLink,
                    context: self.get_context(content, position, 50),
                    position,
                });
            }
        }
        
        Ok(links)
    }

    fn extract_tags(&self, content: &str) -> Vec<String> {
        // ã‚¿ã‚°æŠ½å‡ºã¯è¨˜éŒ²ã®ã¿è¡Œã„ã€é–¢é€£æ€§è¨ˆç®—ã«ã¯ä½¿ç”¨ã—ãªã„
        let tag_regex = Regex::new(r"#([a-zA-Z0-9_-]+)").unwrap();
        tag_regex
            .captures_iter(content)
            .map(|cap| cap.get(1).unwrap().as_str().to_string())
            .collect::<std::collections::HashSet<_>>()
            .into_iter()
            .collect()
    }

    fn generate_slug(&self, file_path: &std::path::Path) -> String {
        file_path
            .file_stem()
            .unwrap()
            .to_string_lossy()
            .to_string()
            .to_lowercase()
            .replace(' ', "-")
    }

    fn generate_slug_from_title(&self, title: &str) -> String {
        title
            .to_lowercase()
            .replace(' ', "-")
            .chars()
            .filter(|c| c.is_alphanumeric() || *c == '-')
            .collect()
    }

    fn get_context(&self, content: &str, position: usize, length: usize) -> String {
        let start = position.saturating_sub(length / 2);
        let end = std::cmp::min(position + length / 2, content.len());
        content[start..end].to_string()
    }

    fn build_link_graph(&self, articles: &[ProcessedArticle]) -> Result<LinkGraphData> {
        let mut graph = HashMap::new();
        let article_slugs: std::collections::HashSet<_> = 
            articles.iter().map(|a| &a.slug).collect();
        
        for article in articles {
            let mut connections = Vec::new();
            
            // Process outbound links (ç›´æ¥ãƒªãƒ³ã‚¯ã®ã¿)
            for link in &article.outbound_links {
                if article_slugs.contains(&link.target_slug) {
                    connections.push(GraphConnection {
                        target: link.target_slug.clone(),
                        connection_type: ConnectionType::DirectLink,
                        bidirectional: false,
                    });
                }
            }
            
            graph.insert(article.slug.clone(), GraphNode {
                connections,
                inbound_count: 0, // Will be calculated in next pass
            });
        }
        
        // Calculate inbound counts
        for article in articles {
            for link in &article.outbound_links {
                if let Some(target_node) = graph.get_mut(&link.target_slug) {
                    target_node.inbound_count += 1;
                }
            }
        }
        
        Ok(LinkGraphData {
            graph,
            generated_at: Utc::now().to_rfc3339(),
            total_connections: graph.values()
                .map(|node| node.connections.len())
                .sum(),
        })
    }

    // ãƒªãƒ³ã‚¯å¼·åº¦ã®æ¦‚å¿µã¯å‰Šé™¤ã—ã€ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ã®èª¿æ•´ã«å¤‰æ›´

    fn validate_content(
        &self,
        articles: &[ProcessedArticle],
        _link_graph: &LinkGraphData,
    ) -> Result<ValidationReport> {
        let mut errors = Vec::new();
        let mut warnings = Vec::new();
        
        let existing_slugs: std::collections::HashSet<_> = 
            articles.iter().map(|a| &a.slug).collect();
        
        // Validate internal links
        for article in articles {
            for link in &article.outbound_links {
                if !existing_slugs.contains(&link.target_slug) {
                    errors.push(ValidationError {
                        error_type: "broken_link".to_string(),
                        source: article.slug.clone(),
                        target: link.target_slug.clone(),
                        context: Some(link.context.clone()),
                    });
                }
            }
            
            // Validate metadata references
            for related_slug in &article.metadata.related_articles {
                if !existing_slugs.contains(related_slug) {
                    warnings.push(ValidationWarning {
                        warning_type: "invalid_related_article".to_string(),
                        source: article.slug.clone(),
                        target: related_slug.clone(),
                        context: None,
                    });
                }
            }
        }
        
        Ok(ValidationReport {
            validation_date: Utc::now().to_rfc3339(),
            total_articles: articles.len(),
            errors,
            warnings,
            summary: ValidationSummary {
                broken_links: errors.len(),
                invalid_references: warnings.len(),
            },
        })
    }

    fn write_articles_data(&self, articles: &[ProcessedArticle]) -> Result<()> {
        let articles_data = ArticlesData {
            articles: articles.to_vec(),
            generated_at: Utc::now().to_rfc3339(),
            total_count: articles.len(),
            home_articles: articles
                .iter()
                .filter(|a| a.metadata.home_display)
                .map(|a| a.slug.clone())
                .collect(),
        };
        
        let output_path = self.output_dir.join("articles.json");
        let json = serde_json::to_string_pretty(&articles_data)?;
        std::fs::write(output_path, json)?;
        
        Ok(())
    }

    fn write_link_graph_data(&self, link_graph: &LinkGraphData) -> Result<()> {
        let output_path = self.output_dir.join("link-graph.json");
        let json = serde_json::to_string_pretty(link_graph)?;
        std::fs::write(output_path, json)?;
        
        Ok(())
    }

    fn write_validation_report(&self, report: &ValidationReport) -> Result<()> {
        let output_path = self.output_dir.join("validation-report.json");
        let json = serde_json::to_string_pretty(report)?;
        std::fs::write(output_path, json)?;
        
        Ok(())
    }
}

// Data structures
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessedArticle {
    pub slug: String,
    pub title: String,
    pub content: String,
    pub metadata: ProcessedMetadata,
    pub file_path: String,
    pub outbound_links: Vec<ProcessedLink>,
    pub inbound_count: usize,
    pub extracted_tags: Vec<String>,
    pub processed_at: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessedMetadata {
    pub title: String,
    pub home_display: bool,
    pub category: Option<String>,
    pub importance: Option<u8>,
    pub related_articles: Vec<String>,
    pub tags: Vec<String>,
    pub created_at: Option<String>,
    pub updated_at: Option<String>,
}

impl Default for ProcessedMetadata {
    fn default() -> Self {
        Self {
            title: "Untitled".to_string(),
            home_display: false,
            category: None,
            importance: Some(3),
            related_articles: Vec::new(),
            tags: Vec::new(),
            created_at: None,
            updated_at: None,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessedLink {
    pub target_slug: String,
    pub link_type: LinkType,
    pub context: String,
    pub position: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum LinkType {
    WikiLink,
    MarkdownLink,
    TagReference,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ArticlesData {
    pub articles: Vec<ProcessedArticle>,
    pub generated_at: String,
    pub total_count: usize,
    pub home_articles: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LinkGraphData {
    pub graph: HashMap<String, GraphNode>,
    pub generated_at: String,
    pub total_connections: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphNode {
    pub connections: Vec<GraphConnection>,
    pub inbound_count: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphConnection {
    pub target: String,
    pub connection_type: ConnectionType,
    pub bidirectional: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ConnectionType {
    DirectLink,
    Bidirectional,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationReport {
    pub validation_date: String,
    pub total_articles: usize,
    pub errors: Vec<ValidationError>,
    pub warnings: Vec<ValidationWarning>,
    pub summary: ValidationSummary,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationError {
    pub error_type: String,
    pub source: String,
    pub target: String,
    pub context: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationWarning {
    pub warning_type: String,
    pub source: String,
    pub target: String,
    pub context: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValidationSummary {
    pub broken_links: usize,
    pub invalid_references: usize,
}
```

### 2. Development Server Integration

#### trunk.toml Configuration
```toml
[build]
target = "index.html"
dist = "dist"

[watch]
watch = ["src", "data"]
ignore = ["target"]

[serve]
address = "127.0.0.1"
port = 8080
open = false

[[hooks]]
stage = "pre_build"
command = "npm"
command_arguments = ["run", "build-data"]
```

#### Development Makefile
```makefile
.PHONY: dev build clean process-articles watch-articles

# ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒã®èµ·å‹•
dev:
	@echo "ğŸš€ Starting development environment..."
	@npm run dev

# è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
process-articles:
	@echo "ğŸ“ Processing articles..."
	@npm run build-data

# è¨˜äº‹ã®ç›£è¦–ãƒ¢ãƒ¼ãƒ‰
watch-articles:
	@echo "ğŸ‘€ Watching articles for changes..."
	@npm run watch-articles

# ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ãƒ“ãƒ«ãƒ‰
build: process-articles
	@echo "ğŸ—ï¸  Building for production..."
	@trunk build --release

# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
clean:
	@echo "ğŸ§¹ Cleaning up..."
	@rm -rf dist data/*.json target

# GitHub Actionsã¨åŒã˜å‡¦ç†ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œ
ci-local: process-articles
	@echo "ğŸ”„ Running CI pipeline locally..."
	@npm run validate-links
	@cargo test
	@trunk build --release
```

## GitHub Workflows

### 1. Article Processing Workflow

#### Trigger Conditions
- Push to main branch with changes in `articles/` directory
- Pull request with article modifications
- Manual workflow dispatch for full rebuild

#### Workflow Steps

```yaml
name: Process Articles
on:
  push:
    paths: ['articles/**/*.md']
  pull_request:
    paths: ['articles/**/*.md']
  workflow_dispatch:

jobs:
  process-articles:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Install Nix
        uses: cachix/install-nix-action@v22
        with:
          github_access_token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Setup development environment
        run: nix develop --command echo "Environment ready"
        
      - name: Process articles
        run: nix develop --command just build-data
        
      - name: Run tests
        run: nix develop --command just test
        
      - name: Commit generated data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: 'Auto-update article data and link graph'
          file_pattern: 'data/*.json'
```

#### Build and Deploy Workflow
```yaml
name: Build and Deploy
on:
  push:
    branches: [main]
  workflow_run:
    workflows: ["Process Articles"]
    types: [completed]

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Install Nix
        uses: cachix/install-nix-action@v22
        with:
          github_access_token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Build application
        run: nix develop --command just build
        
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./dist
```

### 2. Article Processing Scripts

#### process-articles.js
```javascript
const fs = require('fs');
const path = require('path');
const matter = require('gray-matter');
const MarkdownIt = require('markdown-it');

class ArticleProcessor {
  constructor() {
    this.articlesDir = 'articles';
    this.outputDir = 'data';
    this.articles = new Map();
    this.linkGraph = new Map();
  }

  async processAllArticles() {
    // å…¨è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿ãƒ»è§£æ
    const files = this.getMarkdownFiles();
    
    for (const file of files) {
      const article = await this.processArticle(file);
      this.articles.set(article.slug, article);
    }
    
    // ãƒªãƒ³ã‚¯ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
    this.buildLinkGraph();
    
    // JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›
    this.writeArticlesData();
    this.writeLinkGraphData();
  }

  processArticle(filePath) {
    const content = fs.readFileSync(filePath, 'utf8');
    const { data: frontMatter, content: markdown } = matter(content);
    
    const slug = this.generateSlug(filePath);
    const links = this.extractLinks(markdown);
    const tags = this.extractTags(markdown);
    
    return {
      slug,
      title: frontMatter.title || path.basename(filePath, '.md'),
      content: markdown,
      metadata: {
        home_display: frontMatter.home_display || false,
        category: frontMatter.category,
        importance: frontMatter.importance || 3,
        related_articles: frontMatter.related_articles || [],
        tags: [...(frontMatter.tags || []), ...tags],
        created_at: frontMatter.created_at,
        updated_at: frontMatter.updated_at || new Date().toISOString()
      },
      outbound_links: links,
      file_path: filePath
    };
  }

  extractLinks(markdown) {
    const links = [];
    
    // [[è¨˜äº‹å]]å½¢å¼ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    const wikiLinkRegex = /\[\[([^\]]+)\]\]/g;
    let match;
    while ((match = wikiLinkRegex.exec(markdown)) !== null) {
      links.push({
        target_slug: this.generateSlug(match[1]),
        link_type: 'WikiLink',
        context: this.getContext(markdown, match.index),
        position: match.index
      });
    }
    
    // [ãƒ†ã‚­ã‚¹ãƒˆ](slug)å½¢å¼ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
    const markdownLinkRegex = /\[([^\]]+)\]\(([^)]+)\)/g;
    while ((match = markdownLinkRegex.exec(markdown)) !== null) {
      if (!match[2].startsWith('http')) { // å†…éƒ¨ãƒªãƒ³ã‚¯ã®ã¿
        links.push({
          target_slug: match[2],
          link_type: 'MarkdownLink',
          context: this.getContext(markdown, match.index),
          position: match.index
        });
      }
    }
    
    return links;
  }

  extractTags(markdown) {
    const tagRegex = /#([a-zA-Z0-9_-]+)/g;
    const tags = [];
    let match;
    
    while ((match = tagRegex.exec(markdown)) !== null) {
      tags.push(match[1]);
    }
    
    return [...new Set(tags)]; // é‡è¤‡é™¤å»
  }

  buildLinkGraph() {
    const graph = new Map();
    
    for (const [slug, article] of this.articles) {
      if (!graph.has(slug)) {
        graph.set(slug, { connections: [], inbound_count: 0 });
      }
      
      // ã‚¢ã‚¦ãƒˆãƒã‚¦ãƒ³ãƒ‰ãƒªãƒ³ã‚¯ã‚’å‡¦ç†
      for (const link of article.outbound_links) {
        if (this.articles.has(link.target_slug)) {
          graph.get(slug).connections.push({
            target: link.target_slug,
            type: link.link_type,
            strength: this.calculateLinkStrength(article, link)
          });
          
          // ã‚¤ãƒ³ãƒã‚¦ãƒ³ãƒ‰ã‚«ã‚¦ãƒ³ãƒˆã‚’æ›´æ–°
          if (!graph.has(link.target_slug)) {
            graph.set(link.target_slug, { connections: [], inbound_count: 0 });
          }
          graph.get(link.target_slug).inbound_count++;
        }
      }
      
      // ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã®é–¢é€£æ€§ã‚’è¨ˆç®—
      this.addTagBasedConnections(graph, slug, article);
    }
    
    this.linkGraph = graph;
  }

  calculateLinkStrength(fromArticle, link) {
    let strength = 0.5; // ãƒ™ãƒ¼ã‚¹å¼·åº¦
    
    // ãƒªãƒ³ã‚¯ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹èª¿æ•´
    if (link.link_type === 'WikiLink') strength += 0.2;
    if (link.link_type === 'MarkdownLink') strength += 0.1;
    
    // è¨˜äº‹ã®é‡è¦åº¦ã«ã‚ˆã‚‹èª¿æ•´
    strength += (fromArticle.metadata.importance - 3) * 0.1;
    
    return Math.min(1.0, Math.max(0.1, strength));
  }

  addTagBasedConnections(graph, slug, article) {
    for (const [otherSlug, otherArticle] of this.articles) {
      if (slug === otherSlug) continue;
      
      const commonTags = article.metadata.tags.filter(tag => 
        otherArticle.metadata.tags.includes(tag)
      );
      
      if (commonTags.length > 0) {
        const strength = Math.min(0.8, commonTags.length * 0.2);
        
        graph.get(slug).connections.push({
          target: otherSlug,
          type: 'TagBased',
          strength,
          common_tags: commonTags
        });
      }
    }
  }

  writeArticlesData() {
    const articlesData = {
      articles: Array.from(this.articles.values()),
      generated_at: new Date().toISOString(),
      total_count: this.articles.size,
      home_articles: Array.from(this.articles.values())
        .filter(a => a.metadata.home_display)
        .map(a => a.slug)
    };
    
    fs.writeFileSync(
      path.join(this.outputDir, 'articles.json'),
      JSON.stringify(articlesData, null, 2)
    );
  }

  writeLinkGraphData() {
    const linkGraphData = {
      graph: Object.fromEntries(this.linkGraph),
      generated_at: new Date().toISOString(),
      total_connections: Array.from(this.linkGraph.values())
        .reduce((sum, node) => sum + node.connections.length, 0)
    };
    
    fs.writeFileSync(
      path.join(this.outputDir, 'link-graph.json'),
      JSON.stringify(linkGraphData, null, 2)
    );
  }
}

// å®Ÿè¡Œ
const processor = new ArticleProcessor();
processor.processAllArticles().catch(console.error);
```

#### validate-links.js
```javascript
const fs = require('fs');
const path = require('path');

class LinkValidator {
  constructor() {
    this.articlesData = JSON.parse(fs.readFileSync('data/articles.json', 'utf8'));
    this.errors = [];
    this.warnings = [];
  }

  validate() {
    this.validateInternalLinks();
    this.validateMetadataReferences();
    this.generateReport();
  }

  validateInternalLinks() {
    const existingSlugs = new Set(this.articlesData.articles.map(a => a.slug));
    
    for (const article of this.articlesData.articles) {
      for (const link of article.outbound_links) {
        if (!existingSlugs.has(link.target_slug)) {
          this.errors.push({
            type: 'broken_link',
            source: article.slug,
            target: link.target_slug,
            context: link.context
          });
        }
      }
    }
  }

  validateMetadataReferences() {
    const existingSlugs = new Set(this.articlesData.articles.map(a => a.slug));
    
    for (const article of this.articlesData.articles) {
      for (const relatedSlug of article.metadata.related_articles) {
        if (!existingSlugs.has(relatedSlug)) {
          this.warnings.push({
            type: 'invalid_related_article',
            source: article.slug,
            target: relatedSlug
          });
        }
      }
    }
  }

  generateReport() {
    const report = {
      validation_date: new Date().toISOString(),
      total_articles: this.articlesData.articles.length,
      errors: this.errors,
      warnings: this.warnings,
      summary: {
        broken_links: this.errors.filter(e => e.type === 'broken_link').length,
        invalid_references: this.warnings.filter(w => w.type === 'invalid_related_article').length
      }
    };
    
    fs.writeFileSync('data/validation-report.json', JSON.stringify(report, null, 2));
    
    // GitHub Actionsã§ã®è¡¨ç¤ºç”¨
    if (this.errors.length > 0) {
      console.error('âŒ Validation errors found:');
      this.errors.forEach(error => {
        console.error(`  - ${error.type}: ${error.source} -> ${error.target}`);
      });
      process.exit(1);
    }
    
    if (this.warnings.length > 0) {
      console.warn('âš ï¸  Validation warnings:');
      this.warnings.forEach(warning => {
        console.warn(`  - ${warning.type}: ${warning.source} -> ${warning.target}`);
      });
    }
    
    console.log('âœ… Link validation completed successfully');
  }
}

const validator = new LinkValidator();
validator.validate();
```

### 3. Deployment Integration

#### Build Workflow
```yaml
name: Build and Deploy
on:
  push:
    branches: [main]
  workflow_run:
    workflows: ["Process Articles"]
    types: [completed]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          target: wasm32-unknown-unknown
          
      - name: Install wasm-pack
        run: curl https://rustwasm.github.io/wasm-pack/installer/init.sh -sSf | sh
        
      - name: Build WebAssembly
        run: wasm-pack build --target web --out-dir pkg
        
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./dist
```

## Components and Interfaces

### 1. Article Management System

#### DataLoader (GitHub Workflow Integration)
```rust
pub struct DataLoader {
    articles_data: ArticlesData,               // äº‹å‰ç”Ÿæˆã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿
    link_graph_data: LinkGraphData,            // äº‹å‰ç”Ÿæˆã•ã‚ŒãŸãƒªãƒ³ã‚¯ã‚°ãƒ©ãƒ•
    validation_report: ValidationReport,       // ãƒªãƒ³ã‚¯æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ
}

impl DataLoader {
    pub async fn new() -> Result<Self, LoadError>;
    pub async fn load_articles_data() -> Result<ArticlesData, LoadError>;
    pub async fn load_link_graph_data() -> Result<LinkGraphData, LoadError>;
    pub async fn load_validation_report() -> Result<ValidationReport, LoadError>;
    pub fn get_home_articles(&self) -> Vec<&ProcessedArticle>;
    pub fn get_article_by_slug(&self, slug: &str) -> Option<&ProcessedArticle>;
    pub fn get_related_articles(&self, slug: &str) -> Vec<&ProcessedArticle>;
    pub fn get_connection_strength(&self, from: &str, to: &str) -> f32;
}

#[derive(Deserialize)]
pub struct ArticlesData {
    pub articles: Vec<ProcessedArticle>,
    pub generated_at: String,
    pub total_count: usize,
    pub home_articles: Vec<String>,            // home_display=trueã®è¨˜äº‹slugs
}

#[derive(Deserialize)]
pub struct LinkGraphData {
    pub graph: HashMap<String, GraphNode>,     // slug -> connections
    pub generated_at: String,
    pub total_connections: usize,
}

#[derive(Deserialize)]
pub struct ValidationReport {
    pub validation_date: String,
    pub total_articles: usize,
    pub errors: Vec<ValidationError>,
    pub warnings: Vec<ValidationWarning>,
    pub summary: ValidationSummary,
}

#### ArticleManager (Runtime)
```rust
pub struct ArticleManager {
    data_loader: DataLoader,
    articles: HashMap<String, ProcessedArticle>, // slug -> Article
    home_articles: Vec<String>,                // home_display=trueã®è¨˜äº‹ã®slug
    category_index: HashMap<String, Vec<String>>, // category -> slugs
    tag_index: HashMap<String, Vec<String>>,   // tag -> slugs
    link_graph: LinkGraph,                     // è¨˜äº‹é–“ã®ãƒªãƒ³ã‚¯é–¢ä¿‚
}

impl ArticleManager {
    pub async fn new() -> Result<Self, LoadError>;
    pub async fn initialize_from_data_loader() -> Result<Self, LoadError>;
    pub fn get_home_articles(&self) -> Vec<&ProcessedArticle>;
    pub fn get_article_by_slug(&self, slug: &str) -> Option<&ProcessedArticle>;
    pub fn get_related_articles(&self, slug: &str) -> Vec<&ProcessedArticle>;
    pub fn get_relationship_strength(&self, from: &str, to: &str) -> f32; // äº‹å‰è¨ˆç®—æ¸ˆã¿
    pub fn get_validation_status(&self) -> &ValidationReport;
}

// GitHub Workflowã§äº‹å‰å‡¦ç†ã•ã‚ŒãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿
#[derive(Deserialize, Clone)]
pub struct ProcessedArticle {
    pub slug: String,                          // ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç”Ÿæˆ
    pub title: String,                         // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®titleã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«å
    pub content: String,                       // Markdownæœ¬æ–‡
    pub metadata: ProcessedMetadata,           // äº‹å‰å‡¦ç†æ¸ˆã¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    pub file_path: String,                     // å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    pub outbound_links: Vec<ProcessedLink>,    // ã“ã®è¨˜äº‹ã‹ã‚‰ä»–è¨˜äº‹ã¸ã®ãƒªãƒ³ã‚¯
    pub inbound_count: usize,                  // ã“ã®è¨˜äº‹ã‚’å‚ç…§ã—ã¦ã„ã‚‹è¨˜äº‹æ•°
    pub extracted_tags: Vec<String>,           // è¨˜äº‹æœ¬æ–‡ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚°
    pub processed_at: String,                  // å‡¦ç†æ—¥æ™‚
}

// ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§ã®è»½é‡ãªè¨˜äº‹è¡¨ç¾ï¼ˆå¿…è¦ã«å¿œã˜ã¦å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾©å…ƒï¼‰
pub struct Article {
    pub slug: String,
    pub title: String,
    pub content: Option<String>,               // é…å»¶èª­ã¿è¾¼ã¿å¯¾å¿œ
    pub metadata: ProcessedMetadata,
    pub connections: Vec<Connection>,          // äº‹å‰è¨ˆç®—æ¸ˆã¿æ¥ç¶šæƒ…å ±
}

impl Article {
    pub fn from_processed(processed: &ProcessedArticle, connections: Vec<Connection>) -> Sel

#[derive(Debug, Clone)]
pub struct ArticleLink {
    pub target_slug: String,                   // ãƒªãƒ³ã‚¯å…ˆã®slug
    pub link_type: LinkType,                   // ãƒªãƒ³ã‚¯ã®ç¨®é¡
    pub context: String,                       // ãƒªãƒ³ã‚¯å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆ
    pub position: usize,                       // è¨˜äº‹å†…ã§ã®ä½ç½®
}

#[derive(Debug, Clone, PartialEq)]
pub enum LinkType {
    WikiLink,      // [[è¨˜äº‹å]]å½¢å¼
    MarkdownLink,  // [ãƒ†ã‚­ã‚¹ãƒˆ](slug)å½¢å¼
    TagReference,  // #tagå½¢å¼
}
```

#### ArticleLoader
```rust
pub struct ArticleLoader {
    base_path: String,
}

impl ArticleLoader {
    pub fn new(base_path: &str) -> Self;
    pub fn load_all_articles(&self) -> Result<Vec<Article>, LoadError>;
    pub fn load_article_from_file(&self, file_path: &str) -> Result<Article, LoadError>;
    pub fn watch_for_changes(&self) -> impl Stream<Item = ArticleEvent>; // å°†æ¥ã®æ‹¡å¼µç”¨
}

pub enum ArticleEvent {
    Created(String),
    Updated(String),
    Deleted(String),
}
```

### 2. Enhanced Node System

#### NodeType
```rust
pub enum NodeType {
    AuthorProfile {
        image_url: String,
        name: String,
    },
    Article {
        slug: String,
        title: String,
        category: Option<String>,
        importance: u8,
    },
}

pub struct EnhancedNode {
    id: NodeId,
    node_type: NodeType,
    position: Position,
    radius: i32,
    connections: Vec<NodeId>,
}
```

#### NodeFactory
```rust
pub struct NodeFactory;

impl NodeFactory {
    pub fn create_author_node(metadata: &AuthorMetadata) -> EnhancedNode;
    pub fn create_article_node(article: &Article) -> EnhancedNode;
    pub fn calculate_node_size(importance: u8, base_size: i32) -> i32;
    pub fn get_category_color(category: &str) -> String;
}
```

### 3. Enhanced Physics System

#### PhysicsConfiguration
```rust
pub struct PhysicsConfiguration {
    pub force_settings: ForceSettings,
    pub author_node_settings: AuthorNodeSettings,
    pub layout_constraints: LayoutConstraints,
}

pub struct AuthorNodeSettings {
    pub fixed_position: bool,
    pub center_weight: f32, // ä¸­å¿ƒã¸ã®å¼•åŠ›ã®é‡ã¿
}

pub struct LayoutConstraints {
    pub max_distance_from_center: f32,
    pub min_node_distance: f32,
    pub category_clustering: bool,
}
```

### 4. Responsive Layout System

#### ViewportManager
```rust
pub struct ViewportManager {
    pub viewport: Viewport,
    pub device_type: DeviceType,
    pub touch_handler: TouchHandler,
}

pub enum DeviceType {
    Desktop,
    Tablet,
    Mobile,
}

pub struct TouchHandler {
    pub zoom_enabled: bool,
    pub pan_enabled: bool,
    pub node_drag_enabled: bool,
}
```

### 5. Visual Enhancement System

#### NodeRenderer
```rust
pub struct NodeRenderer;

impl NodeRenderer {
    pub fn render_author_node(node: &EnhancedNode) -> Html;
    pub fn render_article_node(node: &EnhancedNode) -> Html;
    pub fn create_tooltip(article: &Article) -> Html;
    pub fn apply_category_styling(category: &str) -> String;
}
```

#### ConnectionRenderer
```rust
pub struct ConnectionRenderer;

impl ConnectionRenderer {
    pub fn render_connections(nodes: &[EnhancedNode]) -> Html;
    pub fn create_animated_line(from: Position, to: Position) -> Html;
    pub fn apply_connection_styling(connection_type: ConnectionType) -> String;
}
```

## Data Models

### Article Data Model

#### Front Matterå½¢å¼
å„Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã«YAMLå½¢å¼ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜è¿°ï¼š

```markdown
---
title: "Rustã§ã®éåŒæœŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°"
home_display: true
category: "programming"
importance: 4
related_articles: ["async-patterns", "tokio-basics"]
tags: ["rust", "async", "programming"]
created_at: "2024-01-15"
updated_at: "2024-01-20"
---

# è¨˜äº‹ã®å†…å®¹
å®Ÿéš›ã®Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã“ã“ã«ç¶šãã¾ã™...
```

#### Rustãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```rust
pub struct ArticleMetadata {
    pub title: String,
    pub home_display: bool,                    // ãƒ›ãƒ¼ãƒ ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ã‹
    pub category: Option<String>,              // ã‚«ãƒ†ã‚´ãƒªï¼ˆè‡ªç”±å…¥åŠ›ï¼‰
    pub importance: Option<u8>,                // 1-5, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3
    pub related_articles: Vec<String>,         // é–¢é€£è¨˜äº‹ã®slugé…åˆ—
    pub tags: Vec<String>,                     // ã‚¿ã‚°é…åˆ—
    pub created_at: Option<String>,            // ä½œæˆæ—¥ï¼ˆISO 8601å½¢å¼ï¼‰
    pub updated_at: Option<String>,            // æ›´æ–°æ—¥ï¼ˆISO 8601å½¢å¼ï¼‰
}

impl Default for ArticleMetadata {
    fn default() -> Self {
        Self {
            title: "Untitled".to_string(),
            home_display: false,               // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯éè¡¨ç¤º
            category: None,
            importance: Some(3),               // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡è¦åº¦
            related_articles: Vec::new(),
            tags: Vec::new(),
            created_at: None,
            updated_at: None,
        }
    }
}
```

#### Front Matterãƒ‘ãƒ¼ã‚µãƒ¼

```rust
pub struct FrontMatterParser;

impl FrontMatterParser {
    pub fn parse(content: &str) -> Result<(ArticleMetadata, String), ParseError> {
        // YAML front matterã‚’è§£æ
        // è¨˜äº‹æœ¬æ–‡ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é›¢
    }
    
    pub fn extract_yaml_block(content: &str) -> Option<&str> {
        // ---ã§å›²ã¾ã‚ŒãŸYAMLãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
    }
    
    pub fn parse_yaml_metadata(yaml: &str) -> Result<ArticleMetadata, ParseError> {
        // YAMLã‚’ArticleMetadataã«å¤‰æ›
    }
}
```

#### ãƒªãƒ³ã‚¯è§£æã‚·ã‚¹ãƒ†ãƒ 

```rust
pub struct LinkExtractor;

impl LinkExtractor {
    pub fn extract_all_links(content: &str) -> Vec<ArticleLink> {
        let mut links = Vec::new();
        links.extend(Self::extract_wiki_links(content));
        links.extend(Self::extract_markdown_links(content));
        links.extend(Self::extract_tag_references(content));
        links
    }
    
    pub fn extract_wiki_links(content: &str) -> Vec<ArticleLink> {
        // [[è¨˜äº‹å]]å½¢å¼ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        // æ­£è¦è¡¨ç¾: \[\[([^\]]+)\]\]
    }
    
    pub fn extract_markdown_links(content: &str) -> Vec<ArticleLink> {
        // [ãƒ†ã‚­ã‚¹ãƒˆ](slug)å½¢å¼ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        // æ­£è¦è¡¨ç¾: \[([^\]]+)\]\(([^)]+)\)
    }
    
    pub fn extract_tag_references(content: &str) -> Vec<ArticleLink> {
        // #tagå½¢å¼ã®ã‚¿ã‚°ã‚’æŠ½å‡º
        // æ­£è¦è¡¨ç¾: #([a-zA-Z0-9_-]+)
    }
    
    pub fn get_link_context(content: &str, position: usize, context_length: usize) -> String {
        // ãƒªãƒ³ã‚¯å‘¨è¾ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    }
}

pub struct LinkGraph {
    connections: HashMap<String, Vec<Connection>>, // slug -> connections
    tag_connections: HashMap<String, Vec<String>>, // tag -> slugs
    relationship_cache: HashMap<(String, String), f32>, // (from, to) -> strength
}

impl LinkGraph {
    pub fn new() -> Self;
    pub fn add_article(&mut self, article: &Article);
    pub fn remove_article(&mut self, slug: &str);
    pub fn get_connections(&self, slug: &str) -> Vec<&Connection>;
    pub fn calculate_relationship_strength(&self, from: &str, to: &str) -> f32;
    pub fn get_related_articles(&self, slug: &str, limit: usize) -> Vec<(String, f32)>;
    pub fn rebuild_cache(&mut self); // é–¢é€£åº¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å†æ§‹ç¯‰
}

#[derive(Debug, Clone)]
pub struct Connection {
    pub target: String,                        // æ¥ç¶šå…ˆã®slug
    pub connection_type: ConnectionType,       // æ¥ç¶šã®ç¨®é¡
    pub strength: f32,                         // æ¥ç¶šã®å¼·ã•ï¼ˆ0.0-1.0ï¼‰
    pub bidirectional: bool,                   // åŒæ–¹å‘æ¥ç¶šã‹ã©ã†ã‹
}

#[derive(Debug, Clone, PartialEq)]
pub enum ConnectionType {
    DirectLink,     // ç›´æ¥ãƒªãƒ³ã‚¯
    TagBased,       // ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã®é–¢é€£
    Bidirectional,  // åŒæ–¹å‘ãƒªãƒ³ã‚¯
}
```

### Node Graph Data Model

```rust
pub struct NodeGraph {
    pub author_node: NodeId,
    pub article_nodes: HashMap<String, NodeId>, // slug -> NodeId
    pub connections: Vec<PhysicsConnection>,
    pub layout_state: LayoutState,
    pub link_graph: Rc<RefCell<LinkGraph>>,    // è¨˜äº‹é–“ã®ãƒªãƒ³ã‚¯æƒ…å ±
}

impl NodeGraph {
    pub fn new(link_graph: Rc<RefCell<LinkGraph>>) -> Self;
    pub fn update_from_link_graph(&mut self);  // LinkGraphã‹ã‚‰ç‰©ç†æ¥ç¶šã‚’æ›´æ–°
    pub fn get_connection_strength(&self, from: NodeId, to: NodeId) -> f32;
    pub fn highlight_related_nodes(&self, node_id: NodeId) -> Vec<NodeId>;
}

pub struct PhysicsConnection {
    pub from: NodeId,
    pub to: NodeId,
    pub connection_type: PhysicsConnectionType,
    pub strength: f32,                         // ç‰©ç†æ¼”ç®—ã§ã®æ¥ç¶šå¼·åº¦
    pub visual_strength: f32,                  // è¦–è¦šçš„ãªç·šã®å¤ªã•
    pub animated: bool,                        // ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³åŠ¹æœ
}

pub enum PhysicsConnectionType {
    AuthorToArticle,
    DirectLink,        // ç›´æ¥ãƒªãƒ³ã‚¯
    TagBased,         // ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ã®é–¢é€£
    Bidirectional,    // åŒæ–¹å‘ãƒªãƒ³ã‚¯
}

pub struct LayoutState {
    pub center_position: Position,
    pub zoom_level: f32,
    pub pan_offset: Position,
    pub highlighted_nodes: Vec<NodeId>,        // ãƒã‚¤ãƒ©ã‚¤ãƒˆä¸­ã®ãƒãƒ¼ãƒ‰
    pub active_connections: Vec<usize>,        // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ¥ç¶šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
}
```

### Configuration Data Model

```rust
pub struct PortfolioConfig {
    pub author: AuthorConfig,
    pub display: DisplayConfig,
    pub physics: PhysicsConfig,
}

pub struct AuthorConfig {
    pub name: String,
    pub image_url: String,
    pub bio: String,
    pub social_links: HashMap<String, String>,
}

pub struct DisplayConfig {
    pub max_home_articles: usize,
    pub default_node_size: i32,
    pub category_colors: HashMap<String, String>,
    pub animation_speed: f32,
}
```

## Error Handling

### Error Types

```rust
#[derive(Debug)]
pub enum PortfolioError {
    ArticleLoadError(String),
    MetadataParseError(String),
    PhysicsError(String),
    RenderError(String),
    ConfigurationError(String),
}

impl std::fmt::Display for PortfolioError {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            PortfolioError::ArticleLoadError(msg) => write!(f, "Article load error: {}", msg),
            PortfolioError::MetadataParseError(msg) => write!(f, "Metadata parse error: {}", msg),
            PortfolioError::PhysicsError(msg) => write!(f, "Physics error: {}", msg),
            PortfolioError::RenderError(msg) => write!(f, "Render error: {}", msg),
            PortfolioError::ConfigurationError(msg) => write!(f, "Configuration error: {}", msg),
        }
    }
}
```

### Error Recovery Strategies

1. **è¨˜äº‹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨˜äº‹ã‚’è¡¨ç¤ºã€ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’è¨˜éŒ²
2. **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã€è­¦å‘Šã‚’è¡¨ç¤º
3. **ç‰©ç†æ¼”ç®—ã‚¨ãƒ©ãƒ¼**: ç‰©ç†æ¼”ç®—ã‚’ä¸€æ™‚åœæ­¢ã€é™çš„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
4. **ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼**: ã‚¨ãƒ©ãƒ¼å¢ƒç•Œã§ã‚­ãƒ£ãƒƒãƒã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
5. **ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼**: æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã€ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
6. **ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã‚¨ãƒ©ãƒ¼**: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ

### Local Development Workflow

#### é–‹ç™ºç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
# 1. ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone <repository-url>
cd portfolio

# 2. Nixé–‹ç™ºç’°å¢ƒã«å…¥ã‚‹
nix develop

# 3. åˆå›ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
just build-data

# 4. é–‹ç™ºã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆè¨˜äº‹ç›£è¦–ä»˜ãï¼‰
just dev
```

#### è¨˜äº‹ä½œæˆãƒ»ç·¨é›†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
```bash
# Nixé–‹ç™ºç’°å¢ƒå†…ã§ä½œæ¥­
nix develop

# 1. æ–°ã—ã„è¨˜äº‹ä½œæˆ
touch articles/new-article.md

# 2. Front matterã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¨˜è¿°
# ---
# title: "æ–°ã—ã„è¨˜äº‹"
# home_display: true
# category: "tech"
# ---

# 3. è¨˜äº‹å‡¦ç†ã‚’å®Ÿè¡Œ
just process-articles

# 4. é–‹ç™ºã‚µãƒ¼ãƒãƒ¼ã§ç¢ºèªï¼ˆè‡ªå‹•ãƒªãƒ­ãƒ¼ãƒ‰ï¼‰
just serve
```

#### ãƒ‡ãƒãƒƒã‚°ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
```bash
# Nixç’°å¢ƒå†…ã§å®Ÿè¡Œ
nix develop

# ãƒªãƒ³ã‚¯æ¤œè¨¼ã®ã¿å®Ÿè¡Œ
just validate-links

# è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ä»˜ãã§å‡¦ç†
cargo run --bin process-articles -- --verbose

# ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
cat data/articles.json | jq '.summary'
cat data/validation-report.json | jq '.errors'

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
just clean && just build-data
```

#### GitHub Actionsã¨ã®åŒæœŸ
```bash
# ãƒ­ãƒ¼ã‚«ãƒ«ã§GitHub Actionsã¨åŒã˜å‡¦ç†ã‚’å®Ÿè¡Œ
nix develop --command just build-data
nix develop --command just test

# ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒŸãƒƒãƒˆ
git add data/*.json
git commit -m "Update article data"
git push
```

#### Nix Shellä½¿ç”¨ä¾‹
```bash
# ä¸€æ™‚çš„ã«Nixç’°å¢ƒã‚’ä½¿ç”¨
nix shell

# ç‰¹å®šã®ã‚³ãƒãƒ³ãƒ‰ã®ã¿Nixç’°å¢ƒã§å®Ÿè¡Œ
nix develop --command just build

# CIç’°å¢ƒã®å†ç¾
nix develop --command bash -c "just build-data && just test && just build"
```

## Testing Strategy

### 1. Unit Tests

#### Article Management
- Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æãƒ†ã‚¹ãƒˆ
- Front matterã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- è¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½

#### Physics System
- ãƒãƒ¼ãƒ‰é–“ã®åŠ›ã®è¨ˆç®—
- è¡çªæ¤œå‡º
- å¢ƒç•Œæ¡ä»¶ã®å‡¦ç†

#### Node Rendering
- å„ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã®æç”»
- ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
- ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ

### 2. Integration Tests

#### End-to-End Workflow
- è¨˜äº‹èª­ã¿è¾¼ã¿ â†’ ãƒãƒ¼ãƒ‰ç”Ÿæˆ â†’ ç‰©ç†æ¼”ç®— â†’ æç”»
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒ‰ãƒ©ãƒƒã‚°ã€ã‚¯ãƒªãƒƒã‚¯ï¼‰
- ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³

#### Performance Tests
- å¤§é‡ãƒãƒ¼ãƒ‰ã§ã®ç‰©ç†æ¼”ç®—æ€§èƒ½
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
- ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ€§èƒ½ã®æ¸¬å®š

### 3. Visual Regression Tests

#### Layout Tests
- ç•°ãªã‚‹ç”»é¢ã‚µã‚¤ã‚ºã§ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
- ãƒãƒ¼ãƒ‰é…ç½®ã®ä¸€è²«æ€§
- ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã®æ»‘ã‚‰ã‹ã•

### 4. Accessibility Tests

#### WCAG Compliance
- ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
- ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ãƒªãƒ¼ãƒ€ãƒ¼å¯¾å¿œ
- ã‚«ãƒ©ãƒ¼ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ

### Test Implementation Framework

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use wasm_bindgen_test::*;

    #[wasm_bindgen_test]
    fn test_article_loading() {
        // ãƒ†ã‚¹ãƒˆå®Ÿè£…
    }

    #[wasm_bindgen_test]
    fn test_node_physics() {
        // ãƒ†ã‚¹ãƒˆå®Ÿè£…
    }

    #[wasm_bindgen_test]
    fn test_responsive_layout() {
        // ãƒ†ã‚¹ãƒˆå®Ÿè£…
    }
}
```

## Performance Considerations

### 1. Physics Optimization
- ç‰©ç†æ¼”ç®—ã®æ›´æ–°é »åº¦ã‚’å‹•çš„ã«èª¿æ•´
- é™æ­¢çŠ¶æ…‹ã®ãƒãƒ¼ãƒ‰ã®è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—
- ç©ºé–“åˆ†å‰²ã«ã‚ˆã‚‹è¡çªæ¤œå‡ºã®æœ€é©åŒ–

### 2. Rendering Optimization
- ä»®æƒ³DOMå·®åˆ†ã®æœ€å°åŒ–
- Canvas/WebGLãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã®æ¤œè¨
- ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã®requestAnimationFrameä½¿ç”¨

### 3. Memory Management
- ä¸è¦ãªãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®è§£æ”¾
- ç‰©ç†ä¸–ç•Œã®å®šæœŸçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
- è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®é…å»¶èª­ã¿è¾¼ã¿

### 4. Bundle Size Optimization
- æœªä½¿ç”¨ã‚³ãƒ¼ãƒ‰ã®é™¤å»
- å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®æ´»ç”¨
- WebAssemblyãƒã‚¤ãƒŠãƒªã®æœ€é©åŒ–